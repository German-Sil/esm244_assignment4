---
title: 'Task 3: Text Wrangling and Analysis'
author: "Germ√°n Silva"
date: "3/8/2022"
output: 
  html_document:
    theme: flatly
    code_folding: hide
    
---

# Overview:

This report looks at text from the first chapter of the _Wetlands_ textbook by . The text is analyzed to visualize the most common words used throughout the text and text sentiment analysis (is wetland science a positive or negative topic?). 

```{r setup, include=TRUE, message = FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

# attach libraries
library(tidyverse)
library(tidytext)
library(textdata)
library(pdftools)
library(ggwordcloud)
```

# Most Common Words
```{r}
# read in the data

wetlands <- pdf_text(here::here("data", "wetlands_chapter_1.pdf"))

wetlands_lines <- data.frame(wetlands) %>% # pdf as data frame
  mutate(page = 1:n()) %>% # added page of occurence
  mutate(text_full = str_split(wetlands, pattern = "\\n")) %>% # split lines by line breaks
  unnest(text_full) %>% # unnest lines
  mutate(text_full = str_trim(text_full)) # trim the edges

wetlands_words <- wetlands_lines %>% # call in the line data
  unnest_tokens(word, text_full) %>% # unnest the text into individual words
  select(-wetlands) 

wetlands_words_clean <- wetlands_words %>% # call words
  anti_join(stop_words, by = "word") %>%  # remove stop words (e.g. and, the, a, etc.)
  filter(!str_detect(word, "[0-9]")) # get rid of numbers 
```

